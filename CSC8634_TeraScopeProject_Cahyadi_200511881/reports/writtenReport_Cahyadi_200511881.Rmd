---
title: "Performance Evaluation Of Terapixel Rendering in Cloud(Super) Computing Project"
author: "Cahyadi/200511881"
date: "1/23/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
knitr::opts_chunk$set(fig.align = "center")
```

# **Introduction**
&nbsp;&nbsp;&nbsp;Cloud technology, a relatively new technology that is based on utilising the internet infrastructure to the fullest has seen a major increase in popularity in recent years. This success could be attributed to the huge growth of the Tech Industries that seen many new Unicorns; a term first mentioned by Aileen Lee on 2013, used to describe privately owned start-up companies that valued more than 1 billion USD. One of many reasons why the emergence of these "Unicorns" of Tech world brought popularity to Cloud technologies is that Cloud technologies provide a huge benefit to a start-up companies to further expanse and scale their business with greater flexibility and relatively minimum cost and commitment. Start-up companies now have an option to rent the necessary hardware infastructure instead of investing huge capital on it, since many of the established Tech companies such as Google and Amazon provide access to their extensive range of hardware to be rent. These infrastructures could then be run via virtualisation methods in which provide a great flexibility of scaling.
  
&nbsp;&nbsp;&nbsp;Cloud technologies have been utilised in many parts of the modern society, one of them being for rendering high-resolution images. The implementation of the cloud technologies in rendering task, would eliminate the hardware constraint for the client or the user. An example of this implementation could be found in rendering model in "Image-Based Network Rendering of Large Meshes for Cloud Computing" (Okamoto et al. 2010). This report would evaluate the performance of GPU nodes in a scalable cloud architecture that are used to produce a Terapixel 3D City visualization of Newcastle Upon Tyne.
  
&nbsp;&nbsp;&nbsp;To gain the information adaquate to create an evaluation on the performance of the GPU nodes capabilities to render the terapixel visualizations, a data mining process following a CRISP-DM best data mining practice would be conducted. The tools that would be used to carry out the data mining process are Rstudio combined with the Project Template package to streamlined the data mining process. 
  
  
# **CRISP-DM Methodology**

## **Business Objective**
&nbsp;&nbsp;&nbsp; The scalable cloud architecture that was created to support the terapixel images rendering in order for it to be accessible for users without putting a constraint on the users hardware. Therefore it might be crucial to be able to extract performance information out of the hardware, in this case, the GPU, to further re-evaluate the amount of GPU nodes and its type needed to run the current rendering process or to create a plan for future usage. The goal of this data mining project is to be able to extract the performance evaluation information out of the current cloud-architecture that is currently use to support the terapixel image rendering. The information extracted would then be able to be used to improve the cost efficiency of running the current cloud-architecture or create future planning to prepare for an expansion. To aid in evaluating the performance of the cloud-architecture rendering process, these questions are provided as a guidance for the analyses. The questions are  

  * Which event dominate the task run time
  * Are there any implication that GPU temperature affect the performance of rendering process
  * Are there any correlation between the increased power draw and render time 
    - Quantify the variation in computation requirements for particular tile
  * Are there any particular GPU cards (based on their serial number) that is distinct from the other in terms of performance
  * Are there any inefficiencies detected for the task scheduling process

&nbsp;&nbsp;&nbsp; To help achieving the goals mentioned above, this data mining project would be done using the best practice CRISP-DM methodology, and would be done entirely on R environment supported by the Project Template package and git version control to further streamline the data mining process. 
  
```{r include = FALSE}
library(ProjectTemplate)
library(gridExtra)
load.project()

# Source the code from analysis script in src folder
source("src/analysis1.R")

```
  
## **Data understanding**

&nbsp;&nbsp;&nbsp; There are three data frames available in which the data mining process could be implemented. These data frames are extracted from the application checkpoint and system metric output from the production of terapixel image. Each data frames are in the comma-separated value format in which it could be loaded directly into R by placing the data frames into the data folder of the Project Template, and loading the project. The three data frames mentioned consist of Application Checkpoint data, GPU data, and Task X Y data. Before any process could be made, an initial data exploration would be done on these data frames. 
  
### _**Application Checkpoints Data**_  

#### _Data Preview_
```{r, echo= FALSE}
head(Copy.of.application.checkpoints)
```
#### _Data Summary_  
```{r , echo= FALSE}
summary(Copy.of.application.checkpoints)
```
&nbsp;&nbsp;&nbsp; The Application Checkpoints Data consist of 660400 rows and 6 columns as could be seen from the first 6 rows of the data and the summary table above. For all the columns that are present on the data frame, all of them are character class, and there are no missing values detected. However, after checking for duplicates, there are `r applicationCheckpointDuplicates` duplicated individual within the data set. Therefore, moving onwards with the analyses, the data would be made sure to carry no duplicates.  
&nbsp;&nbsp;&nbsp; Assessing each columns of the data, a following interpretations could be extracted.   

  * _**Timestamp**_ = Contains the date and the current time of the event down to the milliseconds. The class of this column is that of a character, therefore for further analysis some processing might be needed to convert the timestamp into a numerical format.
  * _**Hostname**_ = Contains the Hostname of the virtual machine. The hostname is auto-assigned by the azure batch system. There are `r length(unique(Copy.of.application.checkpoints$hostname))` hostname recorded on the application checkpoints data.
  * _**Event Name**_ =  Contains the name of the events that are happening on the task. There are 5 distinct events recorded on the application checkpoints data. These events are 
    
    + Total Render = The entire task itself
    + Saving Config = Saving the configuration
    + Render = The rendering process of the image tile
    + Tiling = Post-processing of the rendered tile
    + Uploading = Uploading the output of the post-processed image tile to the Azure Blob Storage
      
  * _**Event Type**_ = Contains two distinct event Type which are STOP and START. This event type are meant to describe the checkpoint of the event name. Meaning, a data point with event name "Tiling" and event type "START" refer to the start of "Tiling" process of a certain task.
  * _**Job ID**_ = Contains the Azure batch job. There are three distinct value of the Job ID column, and it refers to the level of zoom (4,8,12) of the rendering proces.
  * _**Task ID**_ = Contains the ID of the Azure batch task. There are `r length(unique(Copy.of.application.checkpoints$taskId))` distinct task ID. Each task id would have all of the five event names respectively.  
  
  
  
### GPU Data  

#### _Data Preview_
```{r, echo= FALSE}
head(Copy.of.gpu)
```
#### _Data Summary_
```{r, echo= FALSE}
summary(Copy.of.gpu)
```
  
&nbsp;&nbsp;&nbsp; Extracting the information from the data preview and the data summary above, the GPU data consist of 1048575 rows and 8 columns. The columns are consisted of Timestamp, Hostname, GPU serial number, GPU UUID, Power Draw on GPU in Watt, GPU Temperature in Celsius, Percent Utilisation of GPU Cores, and Percent Utilisation of the GPU Memory. For the timestamp, hostname, and GPU UUID, the class of column is listed as characters and for the rest of the columns, they are listed as continuous data. However, upon closer inspection it could be deducted that GPU Serial should not be regarded as a continuous data since it refers to serial number of the physical GPU card. It seems that this data set also suffers from duplicated values, and after some thorough look at the data set there are `r anyDuplicated(Copy.of.gpu)` duplicated values found on the data. Similar to the application checkpoints data, the information regarding the duplicated values within the data should be taken into consideration for further analyses to make sure that these duplicated values are not carried over.  
&nbsp;&nbsp;&nbsp; Assessing each columns of the GPU data, a following interpretations could be extracted.  
  
  * _**Timestamp**_ = Similar to the Application Checkpoints Data,the timestamp column for the GPU data contains the current time in which the condition is being recorded. It shows the date as well as the time down to the milliseconds. There is an interesting feature that is found on the timestamp for the GPU data. The timestamp shows that for each hostname, the GPU condition of each hostname is recorded every **2 Seconds**. 
  * _**Hostname**_ = Similar to the Application Checkpoints data, contains the hostname of the virtual machine that is auto-assigned by the Azure Batch system. The number of unique hostname recorded on this data is equal to the number of hostname that is recorded for the Application Checkpoints data, which is `r length(unique(Copy.of.gpu$hostname))` unique hostnames. Therefore, this information could prove to be useful for the analyses on the later part of this report.
  * _**GPU Serial**_ = Contains the serial number of the physical GPU card. Even though on the data summary the GPU serial column is treated as a continous variable, on reality, this column should be treated as a categorical variable.
  * _**GPU UUID**_ = Contains the unique system id which is assigned by the Azure system to the GPU Unit. Note that GPU UUID is unique for each hostname since the number of unique GPU UUID is equal to that of the number of hostname at `r length(unique(Copy.of.gpu$gpuUUID))` unique data points.
  * _**GPU Power Draw in Watt**_ = Contains the current recorded value of the Power Draw of the GPU in Watt. This variable is a continuous variable. It has a minimum recorded value of 22.55 Watt and a maximum recorded value of 197.01 Watt.
  * _**GPU Temperature in Celcius**_ = Contains the current recorded Temperature of the GPU in Celcius. This variable is a continuous variable in which the minimum recorded value for this variable is 26 degree Celsius and the maximum recorded value of 55 degree Celsius.
  * _**GPU Util Perc**_ = Contains the current recorded value of the Percent Utilisation of the GPU cores. This variable is a continuous variable ranging from 0% to 100%. It has a minimum recorded value of 0% and a maximum recorded value of 100%.
  * _**GPU Memory Util Perc**_ = Contains the current recorded value of the Percent Utilisation of the GPU memory. This variable is continuous variable ranging from 0% to 100%. It has a minimum recorded value of 0% and a maximum recorded value of 83%
  
    
### Task X Y Data  
    
#### _Data Preview_
```{r, echo=FALSE}
head(task.x.y)
```
#### _Data Summary_
```{r, echo=FALSE}
summary(task.x.y)
```
  
&nbsp;&nbsp;&nbsp; Extracting information out of the Data Preview and the Data Summary above, the Task-X-Y Data consist of 6579 rows and 5 columns. The number of rows are equal to the number of task as shown by the taskId column. It has recuring columns from the Application Checkpoints data in taskId and JobId. This information should be taken into consideration when finding the correlation between the Application Checkpoints data and the Task-X-Y data. The Task-X-Y data columns that shows coordinate "X" and "Y" is deemed by the R system as a column of continouus variable, since the values inside the column are consisted of numerical value. However, note that since it only shows the coordinate of an object, the stats value extracted from these two columns might not be that beneficial for the analyses, although it still could be useful for determining whether a certain coordinates affect the performance of the rendering task. The "level" column is shown as a continuous variable on the data summary even though the "level" column represent zoom level of the rendered visualisation. There are no duplicates detected within this data frame.    
&nbsp;&nbsp;&nbsp; Assessing each column of the Task-X-Y data, a following interpretations could be extracted.  

  * _**Task ID**_ = Similar to the Application Checkpoints data, this column contains the ID of the Azure Batch task. Since it shows the same number of unique taskId as that of the Application Checkpoints data with `r length(unique(task.x.y$taskId))` unique task ID, it should be taken into consideration to link the task id from the Task-X-Y data with the task id in the Application Checkpoints data. 
  * _**Job ID**_ = Similar to the Application Checpoints data, this column contains Azure batch job Id
  * _**X**_ = Contains the X coordinate of the rendered image tile.
  * _**Y**_ = Contains the Y coordinate of the rendered image tile.
  * _**Level**_ = Contains the zoom levels of the render. The visualisation is created with 12 zoom levels starting with 1 until 12, however,  for the data set in this project, only zoom level 4,8, and 12 that are present since the intermediate level are derived in the tiling process. Note that the zoom level is also reflected in the JobId column, as jobId also mentions the level of zoom in its character.  
  
  
## **Data Preparation**  
  
&nbsp;&nbsp;&nbsp; Before analyses would be made, the data frames mentioned and described on the Data Understanding section, would need to be processed first. The process includes but not limited to data wrangling, removing duplicates, merging data frames, and extracting new features from the data. In this part of the report, the processes that are applied to the data frames would be discussed and explanations would be given.  
&nbsp;&nbsp;&nbsp; Before any data merging could be done, each individual data frame would be processed first. The data processing is done based on the information gained from the assessment of each data frame that is highlighted in the Data Understanding section of this report.  
  
### _**Application Checkpoints Data**_   
  
&nbsp;&nbsp;&nbsp; Referring to the initial data assessment of the Application Checkpoints data frame, data points duplicates would be removed. Removing the duplicates on the Application Checkpoints would leave us with a data frame which has `r nrow(unique(Copy.of.application.checkpoints))` data points and removing `r anyDuplicated(Copy.of.application.checkpoints)` duplicates. The reason to remove any duplicates is to avoid having any error during the analyses or producing an analysis that is skewed.  
&nbsp;&nbsp;&nbsp; The **"Timestamp"** column of the Application Checkpoints could not be directly used as a method of analyses since the type of values that are present in the column are characters. Therefore we would process the **"Timestamp"** column and extract only the numerical time value out of the **"Timestamp"**. The dates that are recorded on the **"Timestamp"** are all showing the same date which is _**2018-11-08**_ with varying times. Therefore, for the analyses, only the time would be extracted and would be converted into the Hour:Min:Sec format.  
&nbsp;&nbsp;&nbsp; For the **Event Name** column, the number of data points for each event names seems to be the same, indicating that for all the task that is present in the data, each task includes all possible event names. The Distribution table of the **Event Name** column is shown as below  

```{r, echo = FALSE}
applicationCheckpointDistinct = unique(Copy.of.application.checkpoints)
table(applicationCheckpointDistinct$eventName)
```

&nbsp;&nbsp;&nbsp; Each event name recorded on the data 131586 times, which if divided by the number of **Event Type**  of each Event Name which is 2, would equal to the amount of distinct **Task ID**. With these information at hand,we would proceed with wrangling the data, to prepare it for analyses. The wrangling process are done in these manners  
  
  1. Ensure that the Applicaton Checkpoints data does not contains any duplicated data points. If there are any, remove the duplicated data points. 
  2. Extract only the time characters out of the **Timestamp** column and change the format to that of an Hour:Minute:Second format.
  3. Transform the **Event Type** row into two separate columns from the two distinct value of **Event Type** column which are _**START**_ and _**STOP**_ .
  4. Fill these new columns **START** and **STOP** with the **Timestamp** for each **Event Name** which has **Event Type** that matched each of these two new columns. 
  5. Create a new feature called **duration** which would store the duration of each event, which is calculated by subtracting the **STOP** and **START** column. This newly made feature **duration** is what would be used in analyses as a metric that describe the length of an event.   
  
&nbsp;&nbsp;&nbsp; The resulting data frame would be   
```{r, echo=FALSE}
# Call the wrangled application checkpoint data
head(Durations)
```

&nbsp;&nbsp;&nbsp; To aid with the analyses that concerns with the usage of duration column, a horizontal data frame is created in which there are 5 new columns created by using the **Event Name** values as column name and the **duration** of the corresponding **Event Name** would be the values resided within these newly made columns. The resulting data frame would be 
```{r, echo=FALSE}
# Call the horizontal data
print(head(horizontalDurations), options(tibble.width = Inf ))
```
&nbsp;&nbsp;&nbsp; Do note that the newly made columns that are based on the **Event Name** column values are not set up in a chronological order in which for each task, the order of event should be, _**Saving Config**_, _**Render**_, _**Tiling**_, and _**Uploading**_. We would only use this newly made horizontal data to aid in extracting the average duration of each event name to be used for analyses.  
  
  
### _**GPU Data**_   
  
&nbsp;&nbsp;&nbsp; The GPU Data as shown on the Data Understanding part of this report, has some duplicated data points, therefore before any analyses or wrangling is done on this data set, the GPU data set needs to be cleared of this duplicated data points. After removing the data points, the number of rows on the GPU Data would be `r nrow(unique(Copy.of.gpu))` down from the original number of rows for the GPU data of `r nrow(Copy.of.gpu)` data points. Since there are not much information that could be extracted from the GPU data alone, therefore the wrangling that is done for this data set is merely to prepare it for merging this data frame with other data frame on this project. Other than removing any duplicates from the data, the **Timestamp** column of the GPU data would also be transformed into the Hour:Min:Sec format, just as has been done to the **Timestamp** column of the application checkpoints.   
  
### _**Task-X-Y Data**_  
  
&nbsp;&nbsp;&nbsp; Similar to the GPU data frame, the Task-X-Y data by itself, does not have any noticeable information that could be extracted. And since the Task-X-Y data does not contain any duplicated data points, therefore there is no individual wrangling done to this data.  
  
### _**Merging Application Checkpoints Data with GPU Data**_
  
&nbsp;&nbsp;&nbsp; To extract necessary information out of the GPU data, the GPU data needs to be merged with the Application Checkpoints data to gather detailed information regarding the tasks or event tasks that are being run on the GPU nodes at a current point of time. The merged should be by **Hostname** and **Timestamp** columns, however the merge for the **Timestamp** column should not be done so directly, since the **Timestamp** of the GPU data only record the GPU condition every 2 seconds, and if we merged it directly, it would only bear 232 matching results. To tackle this problem, the GPU condition for a particular task is calculated by the average GPU condition during the length of the task itself. An example for this calculation if _**Total Render**_ event started at _**8H 1M 29.305S**_ and ended at _**8H 0M 44.142S**_ we would apply a conditional that would extract all the GPU conditions recorded within this time frame, and for this particular event, we would use the average GPU condition between the two timestamps above to be recorded as the GPU condition of the particular event. With this conditional we are successful on creating the desired data. The data frame that are created are based on the horizontal data, and would bear 40 columns. Since not all the columns would be visible if shown here, below is only a preview of the first view columns of the data that are part of the _**Total Render**_ event name.
#### _Data Preview_
```{r, echo = FALSE}
head(finalData[,1:12])
```

&nbsp;&nbsp;&nbsp; However, after carefully assessing the data, it is found that there is some problem detected on the wrangled data. Since the GPU data **Timestamp** only record the timestamp every 2 seconds, an event name that has duration less than 2 seconds, would return a _**NaN**_ or not a number value. To mitigate this problem, for the GPU conditions that returns _**NaN**_ value would be subjected to another conditional in which, the GPU conditions for this particular event name is average of GPU conditions of the two closest timestamp on the GPU Data to the START and STOP timestamp of this particular event. For example, a _**Saving Config**_ event happen between _**7H 56M 50.424S**_ and _**7H 56M 50.426S**_ and there are no GPU condition that are recorded between that time for the particular hostname. Therefore, we would take the GPU condition of the particular hostname with the timestamp that is earlier than but is the closest to _**7H 56M 50.424S**_ and the one which is latter but closest to _**7H 56M 50.426S**_ and we would take the average between these two closest timestamps and record it as the GPU condition of the corresponding _**Saving Config**_ event. There are both data that has been wrangled horizontally with each event name with its each corresponding features as columns and data that is wrangled vertically meaning that the event names are still contained within the **Event Name** column. But for the preview below, we choose to preview the data that is wrangled vertically so that it is more compact in width. 
```{r, echo = FALSE}
head(verticalData_Raw)
```
  
### _**Merging the Merged Data Frame with Task-X-Y Data**_ 
  
&nbsp;&nbsp;&nbsp; Merging the merged GPU and Application Checkpoints data frame with the Task-X-Y data frame is done by specifying the merge by **Task Id** and **Job Id** columns. For the Horizontal data, the merge would bear 65793 rows while for the vertical data the merge would bear 328965 rows.  

### _**Subsetting the data based on Event Name**_  
  
&nbsp;&nbsp;&nbsp; To streamlined the analyses, the main data would also be subset in terms of its event name and **GPU Serial** column values are extracted as a feature to be recorded into a new column **GPU Serial Feature**. The subset would produce these data frames 

  * totalRenderData = A data frame that contains only event name _**Total Render**_.  
  * eventNamesData = A data frame that contains all event names apart from _**Total Render**_ . In this data frame, the **Event Name** column is also extracted as feature, creating a new feature **Event Name Feature** in which, the event name is turned into a factor, and turned into numerical class for analyses.
  * savingConfigData = A data frame that contains only event name _**Saving Config**_
  * renderData =  A data frame that contains only event name _**Render**_
  * tilingData = A data frame that contains only event name _**Tiling**_
  * uploadingData = A data frame that contains only event name _**Uploading**_

&nbps;&nbsp;&nbsp; These data subsets are created in order for separate analyses for different event name could be executed. 
  
  
## **Data Analyses and Modelling**

&nbsp;&nbsp;&nbsp; To reach the goal and answer the questions mentioned in the earlier part of the report, analyses would be made on the data that we have gathered and processed. The analyses would be done in a structure based on the order of the questions mentioned on the Business Objective part of the report. However before any analyses could be made, and initial analysis would be done on the entire data to provide a broad overview of the data and extract some interesting features.  
  
### _**Numerical Summaries of the Data**_
  
&nbsp;&nbsp;&nbsp Extracting the numerical summaries out of the numerical value of the data would bear us
```{r, echo = FALSE}
summary(verticalData_Raw[,c("duration","AvgPowerDrawWatt","AvgGPUTempC","AvgGPUUtilPerc","AvgGPUMemUtilPerc")])
```
With variation for each column 
```{r, echo=FALSE}
diag(var(verticalData_Raw[,c("duration","AvgPowerDrawWatt","AvgGPUTempC","AvgGPUUtilPerc","AvgGPUMemUtilPerc")]))
```
&nbsp;&nbsp;&nbsp; Based on the numerical summaries extracted from the data as shown above, it seems that out of the numerical columns on the data, the column or feature that has the most variation belong to the **Percent Utilisation of GPU Cores**. The **GPU Temperature** seems to have a really low variation. This information might be used to answer the questions stated on the Business Objective of the report in regards to the GPU Temperature.   
  
### _**Graphical Summaries of the Data**_
```{r, echo = FALSE}

include_graphics("C:/image/complete_eventNamesPairsData.jpeg")
```
&nbsp;&nbsp;&nbsp; From the graph above, most of the feature that relates to GPU performance such as **duration**, **GPU Power Draw**, **Percent Utilisation of GPU Cores** and **Percent Utilisation of GPU Memory** show that data points that belong to _**Render**_ event name tends to reside on the upper side of these graphs. Therefore it might be indicated that _**Render**_ demand more performance on the GPU node than other event name. It seems there are interesting traits based on the correlation value on some of these combination of features plots. These interesting correlation traits are found between these variables  

  * **Duration** and **GPU Power Draw** with a correlation score between the two showing 0.835
  * **Duration** and **Percent Utilisation of GPU Cores** with a correlation score between the two showing 0.844
  * **Duration** and **Percent Utilisation of GPU Memory** with a correlation score between the two showing 0.862
  * **Percent Utilisation of GPU cores** and **GPU Power Draw** with a correlation score between the two showing 0.929
  * **Percent Utilisation of GPU memory** and **GPU Power Draw** with a correlation score between the two showing 0.943
  * **Percent Utilisation of GPU cores** and **Percent Utilisation of GPU memory** with a correlation score between teh two showing 0.984

&nbsp;&nbsp;&nbsp; Apart from the traits shown above, **GPU Temperature** feature doesn't show any strong correlation with any other features shown on the graph above since all the correlation between the **GPU Temperature** and other feature show correlation score below 0.4. These information gathered from plotting the graphs above, would be assessed further to answer the questions stated on the Business Objective part of this report. 
  
  
### **Determining Event which Dominate the Task Run Time**
  
  
&nbsp;&nbsp;&nbsp; For the analysis regarding to the question of which event dominates the task run time, we will use the wrangled application checkpoint data with new feature **duration** as a whole. Meaning that the event name still contains the _**Total Render**_ event. To answer the question regarding which Event dominates the task run time, a box plot would be plotted to show the spread and mean of each event name duration. 
```{r echo= FALSE, fig.height=4, fig.width=6}
ggplot(verticalData_Raw, aes(y=duration, x=eventName, fill = eventName)) + geom_boxplot() + ggtitle("Event Names Run Time") + theme(plot.title = element_text(hjust = 0.5))
```
&nbsp;&nbsp;&nbsp; Based on the graph above, a conclusion could be drawn that the _**Render**_ event name dominates the task run time since the box plot of _**Render**_ event is nearly identical with the _**Total Render**_ event name that represent the whole length of the task run time. To quantify the result of the box plot graph above, a numerical summaries are extracted from the data that contains the duration of each event name.

#### _**Data Preview**_
```{r,echo = FALSE}
head(horizontalDurations)
```
Extracting the mean out of the data would give us a result of 
```{r, echo = FALSE}
colMeans(horizontalDurations[,4:8])
```
With a variance of
```{r, echo = FALSE}
diag(var(horizontalDurations[,4:8]))
```
&nbsp;&nbsp;&nbsp; These result further cemented the narrative in which _**Render**_ event dominate the task run time by more than 90%. 
  
### **Implications on Temperature of GPU Affecting the Rendering Performance**
  
&nbsp;&nbsp;&nbsp; For the analysis used to answer the question regarding the interplay between the GPU temperature and GPU performance, we would use the **Total Render Data** data frame in which the data contains information of the average GPU condition during the length of an event and only contains event name _**Total Render**_ to represent the whole task. GPU capabilities are measured by its performance to succesfully render an object. Since there are no information given on the complexity of the object being rendered, it is assumed that each tile of the rendered object are equal in memory size, therefore the metric in which the GPU performance is measured is in terms of the length of the render process. Therefore, to check on the implications on Temperature of GPU affecting the GPU performance, we would like to revisit the plot that shows the correlation between the Duration of rendering task and the temperature of the GPU during this process. To asses this, we would use only the _**Total Render**_ event name that represent the whole length of the task itself.
```{r ,echo= FALSE , fig.height=4, fig.width=6}
ggplot(totalRenderData, aes(x= duration, y= AvgGPUTempC)) +geom_hex() + ggtitle("GPU Temperature(Celsius) VS Task Duration Graph") + theme(plot.title = element_text(hjust = 0.5)) 
```
&nbsp;&nbsp;&nbsp; Based on the GPU Temperature vs Task Duration graph,it seems that there are no strong correlation between the two variables. The graph didn't show any noticeable trend and the spread of the data appeared randomly scattered and there seems to be a high volume of data points around the mean of both variable. To further quantify this deduction, the correlation matrix between the **GPU Temperature** and the **Duration** of the task show a score of  
```{r, echo= FALSE}
knitr::kable(cor(totalRenderData[,c("duration","AvgGPUTempC")]))
```
  
&nbsp;&nbsp;&nbsp; The deduction from the GPU Temperature vs Task Duration Graph, and further supported by the correlation matrix between the two variables, shows that there are no implications that Temperature of GPU affect the Performance of the GPU rendering capabilities that are measure by the length of the render as performance metric.  
  
### **Connection Between the Increased Power Draw and Render Time and Computational Variation for Particular Tile**
  
&nbsp;&nbsp;&nbsp; For this analysis we would use the **Total Render Data** data frame once again. To answer the question regarding the correlation between an increased power draw and the duration of render, a graph between the two feature variable **GPU Power Draw** and **duration** would be plotted and produce result of
```{r, echo= FALSE, fig.height=4, fig.width=6}
ggplot(totalRenderData, aes(x = duration, y = AvgPowerDrawWatt)) + geom_hex() + ggtitle("Power Draw vs Task Duration Graph") + theme(plot.title = element_text(hjust = 0.5))
```
&nbsp&nbsp&nbsp; Looking at the graph above, there is a slight positive correlation trend is detected between the two variables. However, based on this graph alone, there is no clear indication that increased power draw resulted in longer duration or the other way around. Therefore to better understand the correlation between the two variables, a correlation matrix is produced.
```{r, echo=FALSE}

cor(totalRenderData[,c(7,10)])

```
&nbsp;&nbsp;&nbsp; The correlation matrix shows that there is some correlation between the **GPU Power Draw** and the **Duration** of the render task, however the correlation is not strong, therefore it could be deducted that the **Duration** of an event task does not influence the **Power Draw** or the other way around. Therefore, it prompt a question on what truly drives the increase in **Power Draw** or what caused the render task to run longer. In an attempt to answer these, we would like to perform analysis on the image tile itself (X and Y coordinate) in regards to its rendering time and the variation of the computational requirements for a certain tile.   
&nbsp;&nbsp;&nbsp; First we would like to check each coordinate axis and observe whether there is certain coordinate points that are rendered slowly compared to the average render time of other coordinates within the same axis. Therefore, a hex plot graph is produced
```{r, echo= FALSE, fig.height=4, fig.width=8}
library(grid)
library(gridExtra)
p1 = ggplot(totalRenderData, aes(x = duration, y = x)) + geom_hex()
p2 = ggplot(totalRenderData, aes(x = duration, y = y)) + geom_hex()
grid.arrange(p1,p2,ncol=2,top = textGrob("Tile Coordinates and Its Render time",gp=gpar(fontsize=20,font=3))) 
```
&nbsp;&nbsp;&nbsp; Based on the graphs above, for the X-axis coordinates graph, it seems the tiles that have render time which are more than 70 seconds are X coordinates between 100, and as for the Y-axis coordinates graph, the same could be said the same that there are tile which belongs around the 100 Y-coordinats that took more than 70 seconds to render. However, on the Y-coordinate graph, there are also a small cluster of tiles which took more than 60 second to render. The tiles in question have Y-coordinate around 0.
  
&nbsp;&nbsp;&nbsp; Now to look at computational variance for different tile coordinates we would plot graphs for each computational columns of the data. 

```{r, echo = FALSE, fig.height=4, fig.width=8}
p3 = ggplot(totalRenderData, aes(x = AvgPowerDrawWatt, y = x)) + geom_hex()
p4 = ggplot(totalRenderData, aes(x = AvgPowerDrawWatt, y = y)) + geom_hex()
grid.arrange(p3,p4,ncol=2,top = textGrob("Tile Coordinates and Its GPU Power Draw(Watt)",gp=gpar(fontsize=20,font=3)))
```


```{r, echo = FALSE, fig.height = 4, fig.width= 8}
p5 = ggplot(totalRenderData, aes(x = AvgGPUUtilPerc, y = x)) + geom_hex()
p6 = ggplot(totalRenderData, aes(x = AvgGPUUtilPerc, y = y)) + geom_hex()
grid.arrange(p5,p6,ncol=2,top = textGrob("Tile Coordinates and Its GPU Utilisation(%)",gp=gpar(fontsize=20,font=3)))
```
```{r, echo= FALSE, fig.height=4, fig.width=8}

p7 = ggplot(totalRenderData, aes(x = AvgGPUMemUtilPerc, y = x)) + geom_hex()
p8 = ggplot(totalRenderData, aes(x = AvgGPUMemUtilPerc, y = y)) + geom_hex()
grid.arrange(p7,p8,ncol=2,top = textGrob("Tile Coordinates and Its GPU Memory Utilisation(%)",gp=gpar(fontsize=20,font=3)))

```
&nbsp;&nbsp;&nbsp; Based on the GPU conditions graph in regards to Image tile coordinate above, there is no clear correlation that could be drawn between the position of image tile being rendered and the increase of computational requirement. However, there is one more feature that we could explore, which is the level of render which contributes to the level of zoom of the image.



### **Identifying GPU Card whose Performance Differ from Others**
  
&nbsp;&nbsp;&nbsp There are a total of `r length(unique(totalRenderData$gpuSerial))`  type of physical GPU cards used on the current Cloud Architecture, and we would like to gather the information regarding the performance of each type of GPU cards. To aid in identifying which GPU card perform better or worse than the rest, we would plot a graph in which it will show the average render run time for each type of GPU cards.
```{r echo= FALSE, fig.height=4, fig.width=6}
ggplot(totalRenderData, aes(y = duration, group = gpuSerial, fill = gpuSerial)) + geom_boxplot() + ggtitle("Average Task Duration for Each GPU Serial") + theme(plot.title = element_text(hjust = 0.5)) 

```
&nbsp;&nbsp;&nbsp; From the graph, we could observe that out of the 12 GPU cards type, there are 2 that performs slightly worse than the other which are GPU with serial number of _**3.24117e+11**_ and _**3.24317e+11**_ . To quantify the graph, we extract the average run time for each type of GPU cards. 

```{r, echo = FALSE}
means = aggregate(duration~gpuSerial, totalRenderData, mean)
knitr::kable(means) 
```
&nbsp;&nbsp;&nbsp; From both the graph and the numerical summaries above, we could deduct that out of the 12 distinct GPU Serials, the two GPU Serials that perform the worst are GPU Serial _**3.24117e+11**_ and _**3.24317e+11**_ with average render run time of _**45.36851 Seconds**_ and _**45.29177	Seconds**_ respectively. And the GPU Serial that perform the best is GPU Serial _**3.23617e+11**_ with average render run time of _**41.68215 Seconds**_. The difference might not be much at glance but if we take into account the number of tiles that needs to be rendered which is 655536 tiles in total even after considering all 1024 hostnames working together at a same time, even 1 second difference in average rendering time would result in great inefficiency. Therefore it should be taken into consideration to opt for the fastest type of GPU cards.

### **Efficiency of the Task Scheduling**
  
&nbsp;&nbsp;&nbsp For this analysis, we would use the **Total Render Data** data frame. To extract information regarding the efficiency of task scheduling, we would focus on the **Timestamp** column of the data frame. The data that we would work on is the subset of the **Durations** data frame in which we only interested on the _**START**_ timestamp of the task and the _**STOP**_ timestamp of the task. The inefficiency would be measured by the amount of idle time between tasks. For this analysis we would assess each hostname separately and we would record the total idle time for each hostname. The result of this analysis would be in the form of Inefficiency data found below.  
  
```{r, echo=FALSE}
knitr::kable(inefficiencyData)
```
&nbsp;&nbsp;&nbsp; 



