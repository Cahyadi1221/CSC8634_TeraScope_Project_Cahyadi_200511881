---
title: "Performance Evaluation Of Terapixel Rendering in Cloud(Super) Computing Project"
author: "Cahyadi/200511881"
date: "1/23/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
knitr::opts_chunk$set(fig.align = "center")
```

# **Introduction**
&nbsp;&nbsp;&nbsp;Cloud technology, a relatively new technology that is based on utilising the internet infrastructure to the fullest has seen a major increase in popularity in recent years. This success could be attributed to the huge growth of the Tech Industries that seen many new Unicorns; a term first mentioned by Aileen Lee on 2013, used to describe privately owned start-up companies that valued more than 1 billion USD. One of many reasons why the emergence of these "Unicorns" of Tech world brought popularity to Cloud technologies is that Cloud technologies provide a huge benefit to a start-up companies to further expanse and scale their business with greater flexibility and relatively minimum cost and commitment. Start-up companies now have an option to rent the necessary hardware infastructure instead of investing huge capital on it, since many of the established Tech companies such as Google and Amazon provide access to their extensive range of hardware to be rent. These infrastructures could then be run via virtualisation methods in which provide a great flexibility of scaling.
  
&nbsp;&nbsp;&nbsp;Cloud technologies have been utilised in many parts of the modern society, one of them being for rendering high-resolution images. The implementation of the cloud technologies in rendering task, would eliminate the hardware constraint for the client or the user. An example of this implementation could be found in rendering model in "Image-Based Network Rendering of Large Meshes for Cloud Computing" (Okamoto et al. 2010). This report would evaluate the performance of GPU nodes in a scalable cloud architecture that are used to produce a Terapixel 3D City visualization of Newcastle Upon Tyne.
  
&nbsp;&nbsp;&nbsp;To gain the information adaquate to create an evaluation on the performance of the GPU nodes capabilities to render the terapixel visualizations, a data mining process following a CRISP-DM best data mining practice would be conducted. The tools that would be used to carry out the data mining process are Rstudio combined with the Project Template package to streamlined the data mining process. 
  
  
# **CRISP-DM Methodology**

## **Business Objective**
&nbsp;&nbsp;&nbsp; The scalable cloud architecture that was created to support the terapixel images rendering in order for it to be accessible for users without putting a constraint on the users hardware. Therefore it might be crucial to be able to extract performance information out of the hardware, in this case, the GPU, to further re-evaluate the amount of GPU nodes and its type needed to run the current rendering process or to create a plan for future usage. The goal of this data mining project is to be able to extract the performance evaluation information out of the current cloud-architecture that is currently use to support the terapixel image rendering. The information extracted would then be able to be used to improve the cost efficiency of running the current cloud-architecture or create future planning to prepare for an expansion.  
&nbsp;&nbsp;&nbsp; To help achieving the goals mentioned above, this data mining project would be done using the best practice CRISP-DM methodology, and would be done entirely on R environment supported by the Project Template package and git version control to further streamline the data mining process. 
  
```{r include = FALSE}
library(ProjectTemplate)
load.project()

# Source the code from analysis script in src folder
source("src/analysis1.R")

```
  
## **Data understanding**

&nbsp;&nbsp;&nbsp; There are three data frames available in which the data mining process could be implemented. These data frames are extracted from the application checkpoint and system metric output from the production of terapixel image. Each data frames are in the comma-separated value format in which it could be loaded directly into R by placing the data frames into the data folder of the Project Template, and loading the project. The three data frames mentioned consist of Application Checkpoint data, GPU data, and Task X Y data. Before any process could be made, an initial data exploration would be done on these data frames. 
  
### _**Application Checkpoints Data**_  

#### _Data Preview_
```{r, echo= FALSE}
head(Copy.of.application.checkpoints)
```
#### _Data Summary_  
```{r , echo= FALSE}
summary(Copy.of.application.checkpoints)
```
&nbsp;&nbsp;&nbsp; The Application Checkpoints Data consist of 660400 rows and 6 columns as could be seen from the first 6 rows of the data and the summary table above. For all the columns that are present on the data frame, all of them are character class, and there are no missing values detected. However, after checking for duplicates, there are `r applicationCheckpointDuplicates` duplicated individual within the data set. Therefore, moving onwards with the analyses, the data would be made sure to carry no duplicates.  
&nbsp;&nbsp;&nbsp; Assessing each columns of the data, a following interpretations could be extracted.   

  * _**Timestamp**_ = Contains the date and the current time of the event down to the milliseconds. The class of this column is that of a character, therefore for further analysis some processing might be needed to convert the timestamp into a numerical format.
  * _**Hostname**_ = Contains the Hostname of the virtual machine. The hostname is auto-assigned by the azure batch system. There are `r length(unique(Copy.of.application.checkpoints$hostname))` hostname recorded on the application checkpoints data.
  * _**Event Name**_ =  Contains the name of the events that are happening on the task. There are 5 distinct events recorded on the application checkpoints data. These events are 
    
    + Total Render = The entire task itself
    + Saving Config = Saving the configuration
    + Render = The rendering process of the image tile
    + Tiling = Post-processing of the rendered tile
    + Uploading = Uploading the output of the post-processed image tile to the Azure Blob Storage
      
  * _**Event Type**_ = Contains two distinct event Type which are STOP and START. This event type are meant to describe the checkpoint of the event name. Meaning, a data point with event name "Tiling" and event type "START" refer to the start of "Tiling" process of a certain task.
  * _**Job ID**_ = Contains the Azure batch job. There are three distinct value of the Job ID column, and it refers to the level of zoom (4,8,12) of the rendering proces.
  * _**Task ID**_ = Contains the ID of the Azure batch task. There are `r length(unique(Copy.of.application.checkpoints$taskId))` distinct task ID. Each task id would have all of the five event names respectively.  
  
  
  
### GPU Data  

#### _Data Preview_
```{r, echo= FALSE}
head(Copy.of.gpu)
```
#### _Data Summary_
```{r, echo= FALSE}
summary(Copy.of.gpu)
```
  
&nbsp;&nbsp;&nbsp; Extracting the information from the data preview and the data summary above, the GPU data consist of 1048575 rows and 8 columns. The columns are consisted of Timestamp, Hostname, GPU serial number, GPU UUID, Power Draw on GPU in Watt, GPU Temperature in Celsius, Percent Utilisation of GPU Cores, and Percent Utilisation of the GPU Memory. For the timestamp, hostname, and GPU UUID, the class of column is listed as characters and for the rest of the columns, they are listed as continuous data. However, upon closer inspection it could be deducted that GPU Serial should not be regarded as a continuous data since it refers to serial number of the physical GPU card. It seems that this data set also suffers from duplicated values, and after some thorough look at the data set there are `r anyDuplicated(Copy.of.gpu)` duplicated values found on the data. Similar to the application checkpoints data, the information regarding the duplicated values within the data should be taken into consideration for further analyses to make sure that these duplicated values are not carried over.  
&nbsp;&nbsp;&nbsp; Assessing each columns of the GPU data, a following interpretations could be extracted.  
  
  * _**Timestamp**_ = Similar to the Application Checkpoints Data,the timestamp column for the GPU data contains the current time in which the condition is being recorded. It shows the date as well as the time down to the milliseconds. There is an interesting feature that is found on the timestamp for the GPU data. The timestamp shows that for each hostname, the GPU condition of each hostname is recorded every **2 Seconds**. 
  * _**Hostname**_ = Similar to the Application Checkpoints data, contains the hostname of the virtual machine that is auto-assigned by the Azure Batch system. The number of unique hostname recorded on this data is equal to the number of hostname that is recorded for the Application Checkpoints data, which is `r length(unique(Copy.of.gpu$hostname))` unique hostnames. Therefore, this information could prove to be useful for the analyses on the later part of this report.
  * _**GPU Serial**_ = Contains the serial number of the physical GPU card. Even though on the data summary the GPU serial column is treated as a continous variable, on reality, this column should be treated as a categorical variable.
  * _**GPU UUID**_ = Contains the unique system id which is assigned by the Azure system to the GPU Unit. Note that GPU UUID is unique for each hostname since the number of unique GPU UUID is equal to that of the number of hostname at `r length(unique(Copy.of.gpu$gpuUUID))` unique data points.
  * _**GPU Power Draw in Watt**_ = Contains the current recorded value of the Power Draw of the GPU in Watt. This variable is a continuous variable. It has a minimum recorded value of 22.55 Watt and a maximum recorded value of 197.01 Watt.
  * _**GPU Temperature in Celcius**_ = Contains the current recorded Temperature of the GPU in Celcius. This variable is a continuous variable in which the minimum recorded value for this variable is 26 degree Celsius and the maximum recorded value of 55 degree Celsius.
  * _**GPU Util Perc**_ = Contains the current recorded value of the Percent Utilisation of the GPU cores. This variable is a continuous variable ranging from 0% to 100%. It has a minimum recorded value of 0% and a maximum recorded value of 100%.
  * _**GPU Memory Util Perc**_ = Contains the current recorded value of the Percent Utilisation of the GPU memory. This variable is continuous variable ranging from 0% to 100%. It has a minimum recorded value of 0% and a maximum recorded value of 83%
  
    
### Task X Y Data  
    
#### _Data Preview_
```{r, echo=FALSE}
head(task.x.y)
```
#### _Data Summary_
```{r, echo=FALSE}
summary(task.x.y)
```
  
&nbsp;&nbsp;&nbsp; Extracting information out of the Data Preview and the Data Summary above, the Task-X-Y Data consist of 6579 rows and 5 columns. The number of rows are equal to the number of task as shown by the taskId column. It has recuring columns from the Application Checkpoints data in taskId and JobId. This information should be taken into consideration when finding the correlation between the Application Checkpoints data and the Task-X-Y data. The Task-X-Y data columns that shows coordinate "X" and "Y" is deemed by the R system as a column of continouus variable, since the values inside the column are consisted of numerical value. However, note that since it only shows the coordinate of an object, the stats value extracted from these two columns might not be that beneficial for the analyses, although it still could be useful for determining whether a certain coordinates affect the performance of the rendering task. The "level" column is shown as a continuous variable on the data summary even though the "level" column represent zoom level of the rendered visualisation.  
&nbsp;&nbsp;&nbsp; Assessing each column of the Task-X-Y data, a following interpretations could be extracted.  

  * _**Task ID**_ = Similar to the Application Checkpoints data, this column contains the ID of the Azure Batch task. Since it shows the same number of unique taskId as that of the Application Checkpoints data with `r length(unique(task.x.y$taskId))` unique task ID, it should be taken into consideration to link the task id from the Task-X-Y data with the task id in the Application Checkpoints data. 
  * _**Job ID**_ = Similar to the Application Checpoints data, this column contains Azure batch job Id
  * _**X**_ = Contains the X coordinate of the rendered image tile.
  * _**Y**_ = Contains the Y coordinate of the rendered image tile.
  * _**Level**_ = Contains the zoom levels of the render. The visualisation is created with 12 zoom levels starting with 1 until 12, however,  for the data set in this project, only zoom level 4,8, and 12 that are present since the intermediate level are derived in the tiling process. Note that the zoom level is also reflected in the JobId column, as jobId also mentions the level of zoom in its character.  
  
  
## **Data Preparation**  
  
&nbsp;&nbsp;&nbsp; Before analyses would be made, the data frames mentioned and described on the Data Understanding section, would need to be processed first. The process includes but not limited to data wrangling, removing duplicates, merging data frames, and extracting new features from the data. In this part of the report, the processes that are applied to the data frames would be discussed and explanations would be given.  
&nbsp;&nbsp;&nbsp Before any data merging could be done, each individual data frame would be processed first. The data processing is done based on the information gained from the assessment of each data frame that is highlighted in the Data Understanding section of this report.  
  
### _** Application Checkpoints Data**_
  
&nbsp;&nbsp;&nbsp; Referring to the initial data assessment of the Application Checkpoints data frame, data points duplicates would be removed. Removing the duplicates on the Application Checkpoints would leave us with a data frame which has `r nrow(unique(Copy.of.application.checkpoints))` data points and removing `r anyDuplicated(Copy.of.application.checkpoints)` duplicates. The reason to remove any duplicates is to avoid having any error during the analyses or avoid having a data that is skewed.  



```{r echo=, fig.height=4, fig.width=6}
ggplot(eventNamesData, aes(x= duration, y= AvgGPUTempC, fill = eventName)) +geom_hex()
```

  
```{r, echo = FALSE}
ggplot(eventNamesData, aes(x = duration, y = AvgGPUUtilPerc, fill = eventName)) + geom_hex()
```
  
```{r, echo = FALSE}

include_graphics("C:/image/complete_eventNamesPairsData.jpeg",auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

